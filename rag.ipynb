{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f11c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict, Mapping, Optional\n",
    "import json\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.requests import TextRequestsWrapper\n",
    "from langchain.llms.base import LLM\n",
    "import os\n",
    "from volcengine.maas import MaasService, MaasException, ChatRole\n",
    "\n",
    "# llm = CustomLLM()\n",
    "# print(llm(\"who are you?\"))\n",
    "maas = MaasService('maas-api.ml-platform-cn-beijing.volces.com', 'cn-beijing')\n",
    "maas.set_ak(\"AKLTNjU0MmRhMjRlNTViNGM1OGIwMzE2ZmIzYzQyNWM1ODk\")\n",
    "maas.set_sk(\"T0RjMVpUUTVaRE0xTlRBME5EQXlOMkUzWkdObU56UTFOekkyTlRNd1pqVQ==\")\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "\n",
    "    logging: bool = False\n",
    "    output_keys: List[str] = [\"output\"]\n",
    "\n",
    "    llm_type: str = \"chatglm\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return self.llm_type\n",
    "\n",
    "    def log(self, log_str):\n",
    "        if self.logging:\n",
    "            print(log_str)\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "    def test_chat(self, maas, req):\n",
    "        try:\n",
    "            resp = maas.chat(req)\n",
    "            print(resp)\n",
    "            print(resp.choice.message.content)\n",
    "            return resp.choice.message.content\n",
    "\n",
    "        except MaasException as e:\n",
    "            print(e)\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        self.log('----------' + self._llm_type + '----------> llm._call()')\n",
    "        self.log(prompt)\n",
    "        requests = TextRequestsWrapper()\n",
    "\n",
    "        req = {\n",
    "            \"model\": {\n",
    "                \"name\": \"chatglm2-pro\",\n",
    "            },\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 1024,  # 输出文本的最大tokens限制\n",
    "                \"temperature\": 0.8,  # 用于控制生成文本的随机性和创造性，Temperature值越大随机性越大，取值范围0~1\n",
    "                \"top_p\": 0.8,  # 用于控制输出tokens的多样性，TopP值越大输出的tokens类型越丰富，取值范围0~1\n",
    "                \"top_k\": 16,  # 选择预测值最大的k个token进行采样，取值范围0-1024，0表示不生效\n",
    "            },\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": ChatRole.USER,\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        response = self.test_chat(maas, req)\n",
    "        # response = requests.post(f\"http://js-perf.cn:7001/test/{self._llm_type}\", {\n",
    "        #     \"ask\": prompt\n",
    "        # })\n",
    "        # if self._llm_type == \"chatglm\":\n",
    "        #     self.log('<--------chatglm------------')\n",
    "        #     self.log(response)\n",
    "        #     return response\n",
    "        # else:\n",
    "        #     return \"不支持该类型的 llm\"\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"n\": 10}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e847a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'req_id': '2024042714392469611776F5B8669EF57B', 'choice': {'message': {'content': '因为有蜜蜂'}, 'finish_reason': 'stop'}, 'usage': {'prompt_tokens': 46, 'completion_tokens': 4, 'total_tokens': 50}}\n",
      "因为有蜜蜂\n",
      "{'req_id': '20240427143925F352F53A9B5D4D9B7355', 'choice': {'message': {'content': ' '}}}\n",
      " \n",
      "{'req_id': '20240427143925F352F53A9B5D4D9B7355', 'choice': {'message': {'content': '因为有蜜蜂'}, 'finish_reason': 'stop'}, 'usage': {'prompt_tokens': 46, 'completion_tokens': 4, 'total_tokens': 50}}\n",
      "因为有蜜蜂\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Usage:\n",
    "\n",
    "1. python3 -m pip install --user volcengine\n",
    "2. VOLC_ACCESSKEY=XXXXX VOLC_SECRETKEY=YYYYY python main.py\n",
    "'''\n",
    "import os\n",
    "from volcengine.maas import MaasService, MaasException, ChatRole\n",
    "\n",
    "\n",
    "def test_chat(maas, req):\n",
    "    try:\n",
    "        resp = maas.chat(req)\n",
    "        print(resp)\n",
    "        print(resp.choice.message.content)\n",
    "    except MaasException as e:\n",
    "        print(e)\n",
    "\n",
    "    \n",
    "def test_stream_chat(maas, req):\n",
    "    try:\n",
    "        resps = maas.stream_chat(req)\n",
    "        for resp in resps:\n",
    "            print(resp)\n",
    "            print(resp.choice.message.content)\n",
    "    except MaasException as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    maas = MaasService('maas-api.ml-platform-cn-beijing.volces.com', 'cn-beijing')\n",
    "    \n",
    "    maas.set_ak(\"AKLTNjU0MmRhMjRlNTViNGM1OGIwMzE2ZmIzYzQyNWM1ODk\")\n",
    "    maas.set_sk(\"T0RjMVpUUTVaRE0xTlRBME5EQXlOMkUzWkdObU56UTFOekkyTlRNd1pqVQ==\")\n",
    "    \n",
    "    # document: \"https://www.volcengine.com/docs/82379/1099475\"\n",
    "    req = {\n",
    "        \"model\": {\n",
    "            \"name\": \"chatglm2-pro\",\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 1024,  # 输出文本的最大tokens限制\n",
    "            \"temperature\": 0.8,  # 用于控制生成文本的随机性和创造性，Temperature值越大随机性越大，取值范围0~1\n",
    "            \"top_p\": 0.8,  # 用于控制输出tokens的多样性，TopP值越大输出的tokens类型越丰富，取值范围0~1\n",
    "            \"top_k\": 16,  # 选择预测值最大的k个token进行采样，取值范围0-1024，0表示不生效\n",
    "        },\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": ChatRole.USER,\n",
    "                \"content\": \"天为什么这么蓝？\"\n",
    "            }, {\n",
    "                \"role\": ChatRole.ASSISTANT,\n",
    "                \"content\": \"因为有你\"\n",
    "            }, {\n",
    "                \"role\": ChatRole.USER,\n",
    "                \"content\": \"花儿为什么这么香？\"\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    test_chat(maas, req)\n",
    "    test_stream_chat(maas, req)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1528a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "class ChineseTextSplitter(CharacterTextSplitter):\n",
    "    def __init__(self, pdf: bool = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pdf = pdf\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        if self.pdf:\n",
    "            text = re.sub(r\"\\n{3,}\", \"\\n\", text)\n",
    "            text = re.sub('\\s', ' ', text)\n",
    "            text = text.replace(\"\\n\\n\", \"\")\n",
    "        sent_sep_pattern = re.compile(\n",
    "            '([﹒﹔﹖﹗．。！？][\"’”」』]{0,2}|(?=[\"‘“「『]{1,2}|$))') \n",
    "        sent_list = []\n",
    "        for ele in sent_sep_pattern.split(text):\n",
    "            if sent_sep_pattern.match(ele) and sent_list:\n",
    "                sent_list[-1] += ele\n",
    "            elif ele:\n",
    "                sent_list.append(ele)\n",
    "        return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fbf5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "LOCAL_CONTENT = os.path.join(\"/opt/tiger/rag/docs\")\n",
    "VS_PATH = os.path.join(\"/opt/tiger/rag/vector_store/FAISS\")\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 70\n",
    "VECTOR_SEARCH_TOP_K = 2\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"118c9b24c853f93c5bc138d14dd4af767e9ce6ac41caa211218af4763f9d86f6\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"已知信息：\n",
    "{context}, 回答该问题:{question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "86aec5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict, Mapping, Optional\n",
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "# from models.custom_llm import CustomLLM\n",
    "import datetime\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# from models.config import *\n",
    "from langchain import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation_template = \"\"\"你是一个正在跟某个人类对话的机器人.\n",
    "\n",
    "{chat_history}\n",
    "人类: {human_input}\n",
    "机器人:\"\"\"\n",
    "\n",
    "def load_txt_file(filepath):\n",
    "    loader = TextLoader(filepath, encoding=\"utf8\")\n",
    "    textsplitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE,\n",
    "                                         chunk_overlap=CHUNK_OVERLAP,\n",
    "                                         length_function=len)\n",
    "    docs = loader.load_and_split(text_splitter=textsplitter)\n",
    "    return docs\n",
    "\n",
    "def torch_gc():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        try:\n",
    "            from torch.mps import empty_cache\n",
    "            empty_cache()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"如果您使用的是 macOS 建议将 pytorch 版本升级至 2.0.0 或更高版本，以支持及时清理 torch 产生的内存占用。\")\n",
    "\n",
    "def load_file(filepath):\n",
    "    if filepath.lower().endswith(\".md\"):\n",
    "        loader = UnstructuredFileLoader(filepath, mode=\"elements\")\n",
    "        docs = loader.load()\n",
    "    elif filepath.lower().endswith(\".pdf\"):\n",
    "        loader = UnstructuredFileLoader(filepath)\n",
    "        textsplitter = ChineseTextSplitter(pdf=True)\n",
    "        docs = loader.load_and_split(textsplitter)\n",
    "    else:\n",
    "        docs = load_txt_file(filepath)\n",
    "    return docs\n",
    "\n",
    "def get_related_content(related_docs):\n",
    "    related_content = []\n",
    "    for doc in related_docs:\n",
    "        related_content.append(doc.page_content)\n",
    "    return \"\\n\".join(related_content)\n",
    "\n",
    "def get_docs_with_score(docs_with_score):\n",
    "    docs = []\n",
    "    for doc, score in docs_with_score:\n",
    "        doc.metadata[\"score\"] = score\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "# filepath 可以是目录，也可以是文件\n",
    "def init_knowledge_vector_store(filepath: str or List[str],\n",
    "                                vs_path: str or os.PathLike = None,\n",
    "                                embeddings: object = None):\n",
    "    loaded_files = []\n",
    "    failed_files = []\n",
    "    # 单个文件\n",
    "    if isinstance(filepath, str):\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"{filepath} 路径不存在\")\n",
    "            return None\n",
    "        elif os.path.isfile(filepath):\n",
    "            file = os.path.split(filepath)[-1]\n",
    "            try:\n",
    "                docs = load_file(filepath)\n",
    "                print(f\"{file} 已成功加载\")\n",
    "                loaded_files.append(filepath)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"{file} 未能成功加载\")\n",
    "                return None\n",
    "        elif os.path.isdir(filepath):\n",
    "            docs = []\n",
    "            for file in tqdm(os.listdir(filepath), desc=\"加载文件\"):\n",
    "                fullfilepath = os.path.join(filepath, file)\n",
    "\n",
    "                try:\n",
    "                    docs += load_file(fullfilepath)\n",
    "                    loaded_files.append(fullfilepath)\n",
    "                except Exception as e:\n",
    "                    failed_files.append(file)\n",
    "\n",
    "            if len(failed_files) > 0:\n",
    "                print(\"以下文件未能成功加载：\")\n",
    "                for file in failed_files:\n",
    "                    print(file,end=\"\\n\")\n",
    "    #  文件列表\n",
    "    else:\n",
    "        docs = []\n",
    "        for file in filepath:\n",
    "            try:\n",
    "                docs += load_file(file)\n",
    "                print(f\"{file} 已成功加载\")\n",
    "                loaded_files.append(file)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"{file} 未能成功加载\")\n",
    "\n",
    "    if len(docs) > 0:\n",
    "        print(\"文件加载完毕，正在生成向量库\")\n",
    "        if vs_path and os.path.isdir(vs_path):\n",
    "            vector_store = FAISS.load_local(vs_path, embeddings, allow_dangerous_deserialization=True)\n",
    "            vector_store.add_documents(docs)\n",
    "            torch_gc()\n",
    "        else:\n",
    "            if not vs_path:\n",
    "                vs_path = os.path.join(vs_path,\n",
    "                                       f\"\"\"FAISS_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}\"\"\")\n",
    "            vector_store = FAISS.from_documents(docs, embeddings)\n",
    "            torch_gc()\n",
    "\n",
    "        vector_store.save_local(vs_path)\n",
    "        print(\"向量生成成功\")\n",
    "        return vs_path, loaded_files\n",
    "    else:\n",
    "        print(\"文件均未成功加载，请检查依赖包或替换为其他文件再次上传。\")\n",
    "        return None, loaded_files\n",
    "\n",
    "\n",
    "class LocalDocQA:\n",
    "    filepath: str\n",
    "    vs_path: str\n",
    "    load_files: List[str] = []\n",
    "    top_k: int\n",
    "    embedding: object\n",
    "    llm: object\n",
    "    conversation_with_summary: object\n",
    "    init: bool = True\n",
    "\n",
    "    def __init__(self, filepath: str, vs_path: str, embeddings: object,\n",
    "                       init: bool = True):\n",
    "        if init:\n",
    "            vs_path, loaded_files = init_knowledge_vector_store(filepath=LOCAL_CONTENT,\n",
    "                                                                vs_path=VS_PATH,\n",
    "                                                                embeddings=embeddings)\n",
    "        else:\n",
    "            vs_path = VS_PATH\n",
    "            loaded_files = []\n",
    "\n",
    "\n",
    "        self.load_files = loaded_files\n",
    "        self.vs_path = vs_path\n",
    "        self.filepath = filepath\n",
    "        self.embeddings = embeddings\n",
    "        self.top_k = VECTOR_SEARCH_TOP_K\n",
    "        self.llm = CustomLLM()\n",
    "        self.conversation_with_summary = ConversationChain(llm=self.llm,\n",
    "                                                       memory=ConversationSummaryBufferMemory(llm=self.llm,\n",
    "                                                                                              max_token_limit=40),\n",
    "                                                       verbose=True)\n",
    "\n",
    "    def query_knowledge(self, query: str):\n",
    "        print(self.vs_path)\n",
    "        vector_store = FAISS.load_local(self.vs_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "        vector_store.chunk_size = CHUNK_SIZE\n",
    "        related_docs_with_score = vector_store.similarity_search_with_score(query, k = self.top_k)\n",
    "        related_docs = get_docs_with_score(related_docs_with_score)\n",
    "        related_content = get_related_content(related_docs)\n",
    "        return related_content\n",
    "\n",
    "    def get_knowledge_based_answer(self, query: str):\n",
    "        related_content = self.query_knowledge(query)\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"context\",\"question\"],\n",
    "            template=PROMPT_TEMPLATE,\n",
    "        )\n",
    "        pmt = prompt.format(context=related_content,\n",
    "                            question=query)\n",
    "\n",
    "        # answer=self.conversation_with_summary.predict(input=pmt)\n",
    "        answer = self.llm(pmt)\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1afe1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "RapidAPIKey = \"f1047e5fc5msh235fd70b401935fp1e8db7jsn8ee2b109946b\"\n",
    "\n",
    "class DeepSearch:\n",
    "    def search(self, query=\"\"):\n",
    "        query = query.strip()\n",
    "\n",
    "        if query == \"\":\n",
    "            return \"\"\n",
    "\n",
    "        if RapidAPIKey == \"\":\n",
    "            return \"请配置你的 RapidAPIKey\"\n",
    "\n",
    "        url = \"https://bing-web-search4.p.rapidapi.com/bing-search\"\n",
    "        # print(query)\n",
    "        payload = {\n",
    "            \"keyword\": query,\n",
    "            \"page\": 1,\n",
    "            \"lang\": \"zh-Hans\",\n",
    "            \"region\": \"cn\"\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"X-RapidAPI-Key\": RapidAPIKey,\n",
    "            \"X-RapidAPI-Host\": \"bing-web-search4.p.rapidapi.com\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        # print(response.json())\n",
    "        data_list = response.json()['search_results']\n",
    "\n",
    "        if len(data_list) == 0:\n",
    "            return \"\"\n",
    "        else:\n",
    "            result_arr = []\n",
    "            result_str = \"\"\n",
    "            count_index = 0\n",
    "            for i in range(6):\n",
    "                item = data_list[i]\n",
    "                title = item[\"title\"]\n",
    "                description = item[\"caption\"]\n",
    "                item_str = f\"{title}: {description}\"\n",
    "                result_arr = result_arr + [item_str]\n",
    "\n",
    "            result_str = \"\\n\".join(result_arr)\n",
    "            print(result_str)\n",
    "            return result_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft\n",
      "Microsoft - Official Home Page: 网页At Microsoft our mission and values are to help people and businesses throughout the world realize their full potential.\n",
      "Microsoft - 云、计算机、应用和游戏: 网页Visual Studio 2022. 为使用 Windows 系统的 .NET 和 C++ 开发人员提供功能全面的 IDE，助其构建 web、云、桌面设备应用、移动应用、服务和游戏。. 使用最新的 Microsoft AI 解 …\n",
      "Microsoft account | 立即登录或创建帐户 – Microsoft: 网页Microsoft 帐户连接所有 Microsoft 应用和服务。. 登录以管理帐户。.\n",
      "Microsoft微软官网Surface_Windows_Office_Microsoft 365 ...: 网页微软商店Microsoft Store,Windows官网,购买Surface平板笔记本二合一电脑官方正品,正版Microsoft 365/Win11系统/Office/Office 2021下载激活等尽在微软官方商城.\n",
      "Microsoft 365 - Office 应用的订阅 | Microsoft 365: 网页Microsoft 365 订阅包含位于同一个位置的一套熟悉的 Office 应用、智能云服务和世界级安全性。找到适合你的计划。\n",
      "下载 Windows 10: 网页如果没有安装 Windows 10 的许可证，并且以前尚未升级到此版本，则你可以在此处购买一份：https://www.microsoft.com/zh-cn/windows/get-windows-10. 如果你之前 …\n",
      "Microsoft - Official Home Page: 网页At Microsoft our mission and values are to help people and businesses throughout the world realize their full potential.\n",
      "Microsoft - 云、计算机、应用和游戏: 网页Visual Studio 2022. 为使用 Windows 系统的 .NET 和 C++ 开发人员提供功能全面的 IDE，助其构建 web、云、桌面设备应用、移动应用、服务和游戏。. 使用最新的 Microsoft AI 解 …\n",
      "Microsoft account | 立即登录或创建帐户 – Microsoft: 网页Microsoft 帐户连接所有 Microsoft 应用和服务。. 登录以管理帐户。.\n",
      "Microsoft微软官网Surface_Windows_Office_Microsoft 365 ...: 网页微软商店Microsoft Store,Windows官网,购买Surface平板笔记本二合一电脑官方正品,正版Microsoft 365/Win11系统/Office/Office 2021下载激活等尽在微软官方商城.\n",
      "Microsoft 365 - Office 应用的订阅 | Microsoft 365: 网页Microsoft 365 订阅包含位于同一个位置的一套熟悉的 Office 应用、智能云服务和世界级安全性。找到适合你的计划。\n",
      "下载 Windows 10: 网页如果没有安装 Windows 10 的许可证，并且以前尚未升级到此版本，则你可以在此处购买一份：https://www.microsoft.com/zh-cn/windows/get-windows-10. 如果你之前 …\n"
     ]
    }
   ],
   "source": [
    "ds = DeepSearch()\n",
    "print(ds.search(\"microsoft\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd0154c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "{'req_id': '202404271446106F4862E46CD42A724E67', 'choice': {'message': {'content': '微软是一家世界知名的软件公司,主要业务是开发和销售计算机软件、个人电脑和消费电子产品。微软的旗舰产品包括操作系统和办公软件套件,如Windows和Microsoft Office。微软还开发了许多其他知名软件,如Internet Explorer浏览器、Skype网络电话和Xbox游戏机。微软于1975年创立,总部位于美国华盛顿州西雅图市。'}, 'finish_reason': 'stop'}, 'usage': {'prompt_tokens': 98, 'completion_tokens': 85, 'total_tokens': 183}}\n",
      "微软是一家世界知名的软件公司,主要业务是开发和销售计算机软件、个人电脑和消费电子产品。微软的旗舰产品包括操作系统和办公软件套件,如Windows和Microsoft Office。微软还开发了许多其他知名软件,如Internet Explorer浏览器、Skype网络电话和Xbox游戏机。微软于1975年创立,总部位于美国华盛顿州西雅图市。\n",
      "\u001b[32;1m\u001b[1;3m微软是一家世界知名的软件公司,主要业务是开发和销售计算机软件、个人电脑和消费电子产品。微软的旗舰产品包括操作系统和办公软件套件,如Windows和Microsoft Office。微软还开发了许多其他知名软件,如Internet Explorer浏览器、Skype网络电话和Xbox游戏机。微软于1975年创立,总部位于美国华盛顿州西雅图市。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "微软是一家世界知名的软件公司,主要业务是开发和销售计算机软件、个人电脑和消费电子产品。微软的旗舰产品包括操作系统和办公软件套件,如Windows和Microsoft Office。微软还开发了许多其他知名软件,如Internet Explorer浏览器、Skype网络电话和Xbox游戏机。微软于1975年创立,总部位于美国华盛顿州西雅图市。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools import BaseTool\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "# from models.custom_search import DeepSearch\n",
    "from langchain.agents import BaseSingleActionAgent, AgentOutputParser, LLMSingleActionAgent, AgentExecutor\n",
    "from typing import List, Tuple, Any, Union, Optional, Type\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManagerForToolRun\n",
    "# from models.custom_llm import CustomLLM\n",
    "import re\n",
    "\n",
    "agent_template = \"\"\"\n",
    "你现在是一个{role}。这里是一些已知信息：\n",
    "{related_content}\n",
    "{background_infomation}\n",
    "{question_guide}：{input}\n",
    "\n",
    "{answer_format}\n",
    "\"\"\"\n",
    "\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    template: str\n",
    "    tools: List[Tool]\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        # 没有互联网查询信息\n",
    "        if len(intermediate_steps) == 0:\n",
    "            background_infomation = \"\\n\"\n",
    "            role = \"傻瓜机器人\"\n",
    "            question_guide = \"我现在有一个问题\"\n",
    "            answer_format = \"如果你知道答案，请直接给出你的回答！如果你不知道答案，请你只回答\\\"DeepSearch('搜索词')\\\"，并将'搜索词'替换为你认为需要搜索的关键词，除此之外不要回答其他任何内容。\\n\\n下面请回答我上面提出的问题！\"\n",
    "\n",
    "        # 返回了背景信息\n",
    "        else:\n",
    "            # 根据 intermediate_steps 中的 AgentAction 拼装 background_infomation\n",
    "            background_infomation = \"\\n\\n你还有这些已知信息作为参考：\\n\\n\"\n",
    "            action, observation = intermediate_steps[0]\n",
    "            background_infomation += f\"{observation}\\n\"\n",
    "            role = \"聪明的 AI 助手\"\n",
    "            question_guide = \"请根据这些已知信息回答我的问题\"\n",
    "            answer_format = \"\"\n",
    "\n",
    "        kwargs[\"background_infomation\"] = background_infomation\n",
    "        kwargs[\"role\"] = role\n",
    "        kwargs[\"question_guide\"] = question_guide\n",
    "        kwargs[\"answer_format\"] = answer_format\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "class CustomSearchTool(BaseTool):\n",
    "    name: str = \"DeepSearch\"\n",
    "    description: str = \"\"\n",
    "\n",
    "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None):\n",
    "        return DeepSearch.search(query = query)\n",
    "\n",
    "    async def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"DeepSearch does not support async\")\n",
    "\n",
    "class CustomAgent(BaseSingleActionAgent):\n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        return [\"input\"]\n",
    "\n",
    "    def plan(self, intermedate_steps: List[Tuple[AgentAction, str]],\n",
    "            **kwargs: Any) -> Union[AgentAction, AgentFinish]:\n",
    "        return AgentAction(tool=\"DeepSearch\", tool_input=kwargs[\"input\"], log=\"\")\n",
    "\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # group1 = 调用函数名字\n",
    "        # group2 = 传入参数\n",
    "        match = re.match(r'^[\\s\\w]*(DeepSearch)\\(([^\\)]+)\\)', llm_output, re.DOTALL)\n",
    "\n",
    "        # 如果 llm 没有返回 DeepSearch() 则认为直接结束指令\n",
    "        if not match:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": llm_output.strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # 否则的话都认为需要调用 Tool\n",
    "        else:\n",
    "            action = match.group(1).strip()\n",
    "            action_input = match.group(2).strip()\n",
    "            return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "\n",
    "\n",
    "class DeepAgent:\n",
    "    tool_name: str = \"DeepSearch\"\n",
    "    agent_executor: any\n",
    "    tools: List[Tool]\n",
    "    llm_chain: any\n",
    "\n",
    "    def query(self, related_content: str = \"\", query: str = \"\"):\n",
    "        tool_name = self.tool_name\n",
    "        result = self.agent_executor.run(related_content=related_content, input=query ,tool_name=self.tool_name)\n",
    "        return result\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        llm = CustomLLM()\n",
    "        tools = [\n",
    "                    Tool.from_function(\n",
    "                        func=DeepSearch.search,\n",
    "                        name=\"DeepSearch\",\n",
    "                        description=\"\"\n",
    "                    )\n",
    "                ]\n",
    "        self.tools = tools\n",
    "        tool_names = [tool.name for tool in tools]\n",
    "        output_parser = CustomOutputParser()\n",
    "        prompt = CustomPromptTemplate(template=agent_template,\n",
    "                                      tools=tools,\n",
    "                                      input_variables=[\"related_content\",\"tool_name\", \"input\", \"intermediate_steps\"])\n",
    "\n",
    "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        self.llm_chain = llm_chain\n",
    "\n",
    "        agent = LLMSingleActionAgent(\n",
    "            llm_chain=llm_chain,\n",
    "            output_parser=output_parser,\n",
    "            stop=[\"\\nObservation:\"],\n",
    "            allowed_tools=tool_names\n",
    "        )\n",
    "\n",
    "        agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n",
    "        self.agent_executor = agent_executor\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from custom_llm import CustomLLM\n",
    "\n",
    "    llm = CustomLLM()\n",
    "    tools = [\n",
    "                Tool.from_function(\n",
    "                    func=DeepSearch.search,\n",
    "                    name=\"DeepSearch\",\n",
    "                    description=\"\"\n",
    "                )\n",
    "            ]\n",
    "    tool_names = [tool.name for tool in tools]\n",
    "    output_parser = CustomOutputParser()\n",
    "    prompt = CustomPromptTemplate(template=agent_template,\n",
    "                                  tools=tools,\n",
    "                                  input_variables=[\"related_content\",\"tool_name\", \"input\", \"intermediate_steps\"])\n",
    "\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    agent = LLMSingleActionAgent(\n",
    "        llm_chain=llm_chain,\n",
    "        output_parser=output_parser,\n",
    "        stop=[\"\\nObservation:\"],\n",
    "        allowed_tools=tool_names\n",
    "    )\n",
    "\n",
    "    agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n",
    "    # print(\"hi\")\n",
    "    print(agent_executor.run(related_content=\"\", input=\"介绍一下微软\", tool_name=\"DeepSearch\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name GanymedeNil/text2vec-base-chinese. Creating a new one with MEAN pooling.\n",
      "加载文件: 100%|██████████| 2/2 [00:00<00:00, 123.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件加载完毕，正在生成向量库\n",
      "向量生成成功\n"
     ]
    }
   ],
   "source": [
    "import torch.cuda\n",
    "import torch.backends\n",
    "from typing import Any, List, Dict, Union, Mapping, Optional\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# from models.custom_llm import CustomLLM\n",
    "# from models.custom_agent import DeepAgent\n",
    "# from models.util import LocalDocQA\n",
    "# from models.config import *\n",
    "EMBEDDING_DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "deep_agent = DeepAgent()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"GanymedeNil/text2vec-base-chinese\",\n",
    "                                   model_kwargs={'device':EMBEDDING_DEVICE})\n",
    "\n",
    "qa_doc = LocalDocQA(filepath=LOCAL_CONTENT,\n",
    "                    vs_path=VS_PATH,\n",
    "                    embeddings=embeddings,\n",
    "                    init=True)\n",
    "\n",
    "def answer(query: str = \"\"):\n",
    "    question = query\n",
    "    related_content = qa_doc.query_knowledge(query=question)\n",
    "    formed_related_content = \"\\n\" + related_content\n",
    "    result = deep_agent.query(related_content=formed_related_content, query=question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d90ce52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/tiger/rag/vector_store/FAISS\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "{'req_id': '2024042714574230D4280DBFBF177E4AA8', 'choice': {'message': {'content': '贝拉克·奥巴马（Barack Obama），全名贝拉克·侯赛因·奥巴马二世（Barack Hussein Obama II），生于1961年8月4日，美国政治家、律师，第44任美国总统，于2009年至2017年担任总统。奥巴马是美国历史上第一位非洲裔总统，他的总统任期内，美国经济逐渐从金融危机中复苏，奥巴马政府推行了一系列改革，如奥巴马医保、跨太平洋战略经济伙伴关系协议等。奥巴马卸任后，仍活跃在政治圈，经常发表演讲并会晤其他国家元首。奥巴马信奉基督教，热爱高尔夫球运动。他与夫人米歇尔·奥巴马育有两个女儿，玛丽亚和萨沙。'}, 'finish_reason': 'stop'}, 'usage': {'prompt_tokens': 1078, 'completion_tokens': 152, 'total_tokens': 1230}}\n",
      "贝拉克·奥巴马（Barack Obama），全名贝拉克·侯赛因·奥巴马二世（Barack Hussein Obama II），生于1961年8月4日，美国政治家、律师，第44任美国总统，于2009年至2017年担任总统。奥巴马是美国历史上第一位非洲裔总统，他的总统任期内，美国经济逐渐从金融危机中复苏，奥巴马政府推行了一系列改革，如奥巴马医保、跨太平洋战略经济伙伴关系协议等。奥巴马卸任后，仍活跃在政治圈，经常发表演讲并会晤其他国家元首。奥巴马信奉基督教，热爱高尔夫球运动。他与夫人米歇尔·奥巴马育有两个女儿，玛丽亚和萨沙。\n",
      "\u001b[32;1m\u001b[1;3m贝拉克·奥巴马（Barack Obama），全名贝拉克·侯赛因·奥巴马二世（Barack Hussein Obama II），生于1961年8月4日，美国政治家、律师，第44任美国总统，于2009年至2017年担任总统。奥巴马是美国历史上第一位非洲裔总统，他的总统任期内，美国经济逐渐从金融危机中复苏，奥巴马政府推行了一系列改革，如奥巴马医保、跨太平洋战略经济伙伴关系协议等。奥巴马卸任后，仍活跃在政治圈，经常发表演讲并会晤其他国家元首。奥巴马信奉基督教，热爱高尔夫球运动。他与夫人米歇尔·奥巴马育有两个女儿，玛丽亚和萨沙。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "贝拉克·奥巴马（Barack Obama），全名贝拉克·侯赛因·奥巴马二世（Barack Hussein Obama II），生于1961年8月4日，美国政治家、律师，第44任美国总统，于2009年至2017年担任总统。奥巴马是美国历史上第一位非洲裔总统，他的总统任期内，美国经济逐渐从金融危机中复苏，奥巴马政府推行了一系列改革，如奥巴马医保、跨太平洋战略经济伙伴关系协议等。奥巴马卸任后，仍活跃在政治圈，经常发表演讲并会晤其他国家元首。奥巴马信奉基督教，热爱高尔夫球运动。他与夫人米歇尔·奥巴马育有两个女儿，玛丽亚和萨沙。\n"
     ]
    }
   ],
   "source": [
    "print(answer(\"介绍奥巴马\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d11568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request, make_response\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/ai/langchain/', methods=[\"POST\"])\n",
    "def handle_langchain_ask():\n",
    "    if not request.form or not 'ask' in request.form:\n",
    "        return make_response(jsonify({ \"status\": 500,\n",
    "                                       \"error\":\"error form params\"}), 500)\n",
    "    ask = request.form.get('ask')\n",
    "    content = answer(ask)\n",
    "    return make_response(jsonify({ 'status': 200,\n",
    "                                   'content': content}), 200)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=False, port=8899, host=\"127.0.0.1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
